{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Connect and Import"
      ],
      "metadata": {
        "id": "8YfE6Wr_5T1K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blH7iANCUhUt"
      },
      "outputs": [],
      "source": [
        "# Connect google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3APAnWb9qKx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import multiprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "oos0YGlTkg79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset"
      ],
      "metadata": {
        "id": "a19T5G_Fos9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/deepfake_ds/ds_split_pkl.zip /content/\n",
        "!unzip /content/ds_split_pkl.zip -d /content/ds_split"
      ],
      "metadata": {
        "id": "AOzulTTSophv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class definition\n",
        "\n",
        "class VideoROIDataset(Dataset):\n",
        "    def __init__(self, txt_file, pickle_dir, buffer_size=5, batch_size=100):\n",
        "        self.pickle_dir = pickle_dir\n",
        "        self.buffer_size = buffer_size                                          # N. of pkl files to load in the buffer\n",
        "        self.batch_size = batch_size                                            # N. of video_data for each pkl batch\n",
        "        self.lock = multiprocessing.Lock()                                      # Lock for sync\n",
        "\n",
        "        # Read video name from .txt\n",
        "        with open(txt_file, \"r\") as f:\n",
        "            self.video_list = [line.strip().split(\",\")[0] for line in f]\n",
        "\n",
        "        # Sort Pickle files\n",
        "        self.pickle_files = sorted(\n",
        "            [os.path.join(self.pickle_dir, f) for f in os.listdir(self.pickle_dir) if f.endswith(\".pkl\")],\n",
        "            key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0])\n",
        "        )\n",
        "        self.total_videos = len(self.video_list)\n",
        "        self.data = []\n",
        "        self.current_file_idx = 0\n",
        "        self._load_next_buffer()\n",
        "\n",
        "    def _load_next_buffer(self):\n",
        "        with self.lock:\n",
        "            if len(self.data) > self.batch_size:                                # Buffer refresh check\n",
        "                return\n",
        "\n",
        "            if self.current_file_idx >= len(self.pickle_files):\n",
        "                print(\"All pickle files loaded.\")\n",
        "                return\n",
        "\n",
        "            # Load next N when buffer contains less than batch_size video_data\n",
        "            files_loaded = 0\n",
        "            while files_loaded < self.buffer_size - 1 and self.current_file_idx < len(self.pickle_files):\n",
        "                file_path = self.pickle_files[self.current_file_idx]\n",
        "                print(f\"Loading file: {file_path}\")\n",
        "                with open(file_path, \"rb\") as f:\n",
        "                    self.data.extend(pickle.load(f))\n",
        "                self.current_file_idx += 1\n",
        "                files_loaded += 1\n",
        "\n",
        "            if not self.data:\n",
        "                raise IndexError(\"No available data\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if not self.data or index >= len(self.data):\n",
        "            self._load_next_buffer()                                            # Load new data if necessary\n",
        "            if not self.data:\n",
        "                raise IndexError(\"Buffer empty after loading!\")\n",
        "\n",
        "        video_data = self.data[index % len(self.data)]\n",
        "        eyes = torch.stack(video_data[\"eyes\"])\n",
        "        nose = torch.stack(video_data[\"nose\"])\n",
        "        mouth = torch.stack(video_data[\"mouth\"])\n",
        "        label = torch.tensor(video_data[\"label\"], dtype=torch.float32)\n",
        "\n",
        "        return {\"eyes\": eyes, \"nose\": nose, \"mouth\": mouth, \"label\": label}\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_videos\n",
        "\n",
        "# Buffered dataloader class definition, for dinamic loading of batches\n",
        "class BufferedDataLoader(DataLoader):\n",
        "    def __init__(self, dataset, *args, **kwargs):\n",
        "        super().__init__(dataset, *args, **kwargs)\n",
        "\n",
        "    def __iter__(self):\n",
        "        dataset = self.dataset                                                  # Dataset\n",
        "        for batch in super().__iter__():\n",
        "            # Check buffer dimension and load new data if necessary\n",
        "            dataset._load_next_buffer()\n",
        "            yield batch"
      ],
      "metadata": {
        "id": "e9FN2z-YwGwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Video list directories\n",
        "train_txt = \"/content/ds_split/train_ds.txt\"\n",
        "test_txt = \"/content/ds_split/test_ds.txt\"\n",
        "val_txt = \"/content/ds_split/val_ds.txt\"\n",
        "\n",
        "# Batch directories\n",
        "train_batches_dir = \"/content/ds_split/train_batches\"\n",
        "test_batches_dir = \"/content/ds_split/test_batches\"\n",
        "val_batches_dir = \"/content/ds_split/val_batches\"\n",
        "\n",
        "# Create Datasets\n",
        "train_dataset = VideoROIDataset(train_txt, train_batches_dir)\n",
        "test_dataset = VideoROIDataset(test_txt, test_batches_dir)\n",
        "val_dataset = VideoROIDataset(val_txt, val_batches_dir)\n",
        "\n",
        "# Create Dataloaders\n",
        "dl_batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=dl_batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=dl_batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=dl_batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"DataLoaders created succesfully!\")"
      ],
      "metadata": {
        "id": "EIrgMGrBMhRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX2vyS3vTcjI"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "DMz__vcqfXey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "X8zxbwS9rh85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class MultiInputMobileNetLSTM(nn.Module):\n",
        "    def __init__(self, num_frames=20, num_classes=2, class_weights=None):\n",
        "        super(MultiInputMobileNetLSTM, self).__init__()\n",
        "        self.num_frames = num_frames\n",
        "\n",
        "        # Backbone MobileNetV2 for each ROI\n",
        "        self.backbone_eyes = self._initialize_mobilenet()\n",
        "        self.backbone_nose = self._initialize_mobilenet()\n",
        "        self.backbone_mouth = self._initialize_mobilenet()\n",
        "\n",
        "        # GAP Layer\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # LSTM for time analysis\n",
        "        feature_dim = 1280;\n",
        "        self.lstm = nn.LSTM(input_size=feature_dim * 3, hidden_size=512, num_layers=1, batch_first=True)\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(512, 1)\n",
        "\n",
        "        # Add class weights if given\n",
        "        if class_weights is not None:\n",
        "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "        else:\n",
        "            self.class_weights = None\n",
        "\n",
        "    # Initialize MobileNetV2 with first frozen layers\n",
        "    def _initialize_mobilenet(self):\n",
        "        mobilenet = models.mobilenet_v2(pretrained=True, width_mult=1)\n",
        "\n",
        "        # Freeze first 14 layers\n",
        "        for idx, layer in enumerate(mobilenet.features):\n",
        "            if idx < 14:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "        return nn.Sequential(*mobilenet.features)\n",
        "\n",
        "\n",
        "    def forward(self, eyes, nose, mouth):\n",
        "        batch_size = eyes.size(0) // self.num_frames\n",
        "\n",
        "        # Extract feature through backbone MobileNet\n",
        "        eyes_features = self.backbone_eyes(eyes)  # [B*num_frames, 1280, 1, 1]\n",
        "        nose_features = self.backbone_nose(nose)\n",
        "        mouth_features = self.backbone_mouth(mouth)\n",
        "\n",
        "        # Apply Global Average Pooling\n",
        "        eyes_features = self.gap(eyes_features).squeeze(-1).squeeze(-1)  # [B*num_frames, 1280]\n",
        "        nose_features = self.gap(nose_features).squeeze(-1).squeeze(-1)\n",
        "        mouth_features = self.gap(mouth_features).squeeze(-1).squeeze(-1)\n",
        "\n",
        "        # Resize to proper dimensions\n",
        "        eyes_features = eyes_features.view(batch_size, self.num_frames, -1)  # [B, num_frames, 1280]\n",
        "        nose_features = nose_features.view(batch_size, self.num_frames, -1)\n",
        "        mouth_features = mouth_features.view(batch_size, self.num_frames, -1)\n",
        "\n",
        "        # Concatenation\n",
        "        ROI_features = torch.cat((eyes_features, nose_features, mouth_features), dim=2)  # [B, num_frames, 3840]\n",
        "\n",
        "        # LSTM for time analysis\n",
        "        lstm_out, _ = self.lstm(ROI_features)  # [B, num_frames, 512]\n",
        "\n",
        "        # Use only last frame output for classification\n",
        "        lstm_final_output = lstm_out[:, -1, :]  # [B, 512]\n",
        "\n",
        "        # Fully connected layer\n",
        "        logits = self.fc(lstm_final_output)  # [B, 1]\n",
        "\n",
        "        return logits.squeeze()  # [B]\n"
      ],
      "metadata": {
        "id": "Ge62wH-_RBuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model upgrade"
      ],
      "metadata": {
        "id": "SfOL8nscEkh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class MultiInputMobileNetLSTM(nn.Module):\n",
        "    def __init__(self, config, num_frames=20, num_classes=2, class_weights=None):\n",
        "        super(MultiInputMobileNetLSTM, self).__init__()\n",
        "        self.config = config\n",
        "        self.num_frames = num_frames\n",
        "\n",
        "        # Backbone MobileNetV2 for each ROI\n",
        "        self.backbone_eyes = self._initialize_mobilenet()\n",
        "        self.backbone_nose = self._initialize_mobilenet()\n",
        "        self.backbone_mouth = self._initialize_mobilenet()\n",
        "\n",
        "        # GAP Layer\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Batch Normalization after MobileNet\n",
        "        self.bn_eyes = nn.BatchNorm1d(1280)\n",
        "        self.bn_nose = nn.BatchNorm1d(1280)\n",
        "        self.bn_mouth = nn.BatchNorm1d(1280)\n",
        "\n",
        "        # Dropout after concatenation\n",
        "        self.dropout_ROI = nn.Dropout(p=self.config[\"dropout1\"])\n",
        "\n",
        "        # Fully connected layer to reduce dimension after concatenation\n",
        "        feature_dim = 1280\n",
        "        self.fc_ROI = nn.Linear(feature_dim * 3, feature_dim)\n",
        "        self.ln_pre_LSTM=nn.LayerNorm(feature_dim)\n",
        "\n",
        "        # LSTM for time analysis\n",
        "        self.lstm = nn.LSTM(input_size=feature_dim, hidden_size=512, num_layers=1, batch_first=True)\n",
        "\n",
        "        # Layer Normalization pre LSTM\n",
        "        self.ln_post_lstm = nn.LayerNorm(512)\n",
        "        self.dropout_lstm = nn.Dropout(p=self.config[\"dropout2\"])\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(512, 1)\n",
        "\n",
        "        # Add class weights if given\n",
        "        if class_weights is not None:\n",
        "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "        else:\n",
        "            self.class_weights = None\n",
        "\n",
        "    # Initialize MobileNetV2 for training\n",
        "    def _initialize_mobilenet(self):\n",
        "        if self.config[\"backbone_dim\"] == 1:\n",
        "            mobilenet = models.mobilenet_v2(pretrained=True, width_mult=1)\n",
        "\n",
        "            # Freeze first 14 layers\n",
        "            for idx, layer in enumerate(mobilenet.features):\n",
        "                if idx < 14:\n",
        "                    for param in layer.parameters():\n",
        "                        param.requires_grad = False\n",
        "\n",
        "            return nn.Sequential(*mobilenet.features)\n",
        "\n",
        "        else:\n",
        "            width_mult = self.config[\"backbone_dim\"]\n",
        "            mobilenet = models.mobilenet_v2(pretrained=False, width_mult=width_mult)\n",
        "\n",
        "            # Full training (no pretrained)\n",
        "            for param in mobilenet.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "            return nn.Sequential(*mobilenet.features)\n",
        "\n",
        "    def forward(self, eyes, nose, mouth):\n",
        "        batch_size = eyes.size(0) // self.num_frames\n",
        "\n",
        "        # Extract feature through backbone MobileNet\n",
        "        eyes_features = self.backbone_eyes(eyes)                              # [B*num_frames, 1280, 2, 2]\n",
        "        nose_features = self.backbone_nose(nose)\n",
        "        mouth_features = self.backbone_mouth(mouth)\n",
        "\n",
        "        # Apply Global Average Pooling\n",
        "        eyes_features = self.gap(eyes_features).squeeze(-1).squeeze(-1)       # [B*num_frames, 1280]\n",
        "        nose_features = self.gap(nose_features).squeeze(-1).squeeze(-1)\n",
        "        mouth_features = self.gap(mouth_features).squeeze(-1).squeeze(-1)\n",
        "\n",
        "        # Apply BatchNorm\n",
        "        if self.config[\"batch_norm\"]:\n",
        "            eyes_features = self.bn_eyes(eyes_features)                           # [B*num_frames, 1280]\n",
        "            nose_features = self.bn_nose(nose_features)\n",
        "            mouth_features = self.bn_mouth(mouth_features)\n",
        "\n",
        "        # Resize to proper dimensions\n",
        "        eyes_features = eyes_features.view(batch_size, self.num_frames, -1)   # [B, num_frames, 1280]\n",
        "        nose_features = nose_features.view(batch_size, self.num_frames, -1)\n",
        "        mouth_features = mouth_features.view(batch_size, self.num_frames, -1)\n",
        "\n",
        "        # Concatenation and Dropout\n",
        "        ROI_features = torch.cat((eyes_features, nose_features, mouth_features), dim=2)  # [B, num_frames, 1280 * 3]\n",
        "        ROI_features = self.dropout_ROI(ROI_features)\n",
        "\n",
        "        # Fully connected layer\n",
        "        ROI_features = self.fc_ROI(ROI_features)                                # [B, num_frames, 1280]\n",
        "        if self.config[\"layer_norm1\"]:\n",
        "            ROI_features = self.ln_pre_LSTM(ROI_features)\n",
        "\n",
        "        # LSTM for time analysis\n",
        "        lstm_out, _ = self.lstm(ROI_features)                            # [B, num_frames, 512]\n",
        "\n",
        "        # Use only last frame output for classification\n",
        "        lstm_final_output = lstm_out[:, -1, :]                                # [B, 512]\n",
        "        if self.config[\"layer_norm2\"]:\n",
        "            lstm_final_output = self.ln_post_lstm(lstm_final_output)\n",
        "        lstm_final_output = self.dropout_lstm(lstm_final_output)\n",
        "\n",
        "        # Fully connected layer\n",
        "        logits = self.fc(lstm_final_output)                                   # [B, 1]\n",
        "\n",
        "        return logits.squeeze()                                        # [B]\n"
      ],
      "metadata": {
        "id": "r_u4rhRjEnl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "v8VLSfMkSsbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility"
      ],
      "metadata": {
        "id": "v-FFvLHX0pEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.75, gamma=2.0, reduction='mean'):              # if main class is 1 (fake), alfa> 0.5\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Calculate probabilities\n",
        "        probas = torch.sigmoid(inputs)\n",
        "        pt = probas * targets + (1 - probas) * (1 - targets)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = -self.alpha * (1 - pt) ** self.gamma * torch.log(pt + 1e-12)  # Evita log(0)\n",
        "\n",
        "        # Reduction\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "# Early Stopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_loss = float(\"inf\")\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True"
      ],
      "metadata": {
        "id": "8pmbpvb_le-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train loop"
      ],
      "metadata": {
        "id": "GpPt67w6yYmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "kMhV5neN8UHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path for checkpoints\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/deepfake_ds/modeep_best_models/\"\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "train_session = 40                                                             # Manually change this parameter for each training session\n",
        "\n",
        "loss_choice = \"Focal\"                                                            # Focal or BCE\n",
        "\n",
        "# Parameters\n",
        "ok_config = {\n",
        "    \"dropout1\": 0.6,\n",
        "    \"dropout2\": 0.8,\n",
        "    \"batch_norm\": False,\n",
        "    \"layer_norm1\": False,\n",
        "    \"layer_norm2\": True,\n",
        "    \"backbone_dim\": 1\n",
        "}\n",
        "\n",
        "# Model initialization\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultiInputMobileNetLSTM(ok_config)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "if loss_choice == \"Focal\":\n",
        "    criterion = FocalLoss(alpha=0.85, gamma=3, reduction='mean')\n",
        "else:\n",
        "    total_real = 826\n",
        "    total_fake = 5414\n",
        "    pos_weight = torch.tensor([total_real / total_fake], dtype=torch.float32).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=5e-6, weight_decay=1e-2)             # Optimizer with L2 reg (weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=10,                                                               # Numero di epoche per completare il ciclo\n",
        "    eta_min=1e-7                                                            # Valore minimo del learning rate\n",
        ")\n",
        "\n",
        "\n",
        "# Training configuration\n",
        "num_epochs = 30\n",
        "early_stopping = EarlyStopping(patience=7)                                    # Early stopping\n",
        "\n",
        "# Metrics lists\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "val_aucs = []\n",
        "\n",
        "# Training loop\n",
        "best_val_loss = float(\"inf\")\n",
        "best_val_auc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "\n",
        "        # Load data\n",
        "        eyes = batch[\"eyes\"].view(-1, 3, 64, 64).to(device)\n",
        "        nose = batch[\"nose\"].view(-1, 3, 64, 64).to(device)\n",
        "        mouth = batch[\"mouth\"].view(-1, 3, 64, 64).to(device)\n",
        "        labels = batch[\"label\"].float().to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(eyes, nose, mouth)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {avg_train_loss:.5f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    val_loss = 0.0\n",
        "    all_labels = []                                                             # AUC\n",
        "    all_probs = []                                                              # AUC\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_batch in val_loader:\n",
        "\n",
        "            # Load data\n",
        "            eyes = val_batch[\"eyes\"].view(-1, 3, 64, 64).to(device)\n",
        "            nose = val_batch[\"nose\"].view(-1, 3, 64, 64).to(device)\n",
        "            mouth = val_batch[\"mouth\"].view(-1, 3, 64, 64).to(device)\n",
        "            labels = val_batch[\"label\"].float().to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(eyes, nose, mouth)\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "            # Save probabilities and labels for AUC\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "            # Accuracy\n",
        "            predicted = (probs > 0.5).long()\n",
        "            total_correct += (predicted == labels.long()).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = total_correct / total_samples\n",
        "    val_auc = roc_auc_score(all_labels, all_probs)                 # AUC calculation\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Validation Loss: {avg_val_loss:.5f}, \"\n",
        "          f\"Accuracy: {val_accuracy:.4f}, AUC: {val_auc:.4f}\")\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print learning rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        print(f\"Learning Rate: {param_group['lr']}\")\n",
        "\n",
        "    # Save metrics\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    val_aucs.append(val_auc)\n",
        "\n",
        "    # Save model if it's better\n",
        "    if avg_val_loss < best_val_loss:                                            # or val_auc > best_val_auc\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_val_auc = val_auc\n",
        "        torch.save(model.state_dict(), f\"{CHECKPOINT_PATH}best_model_{train_session}.pth\")\n",
        "        print(f\"Saved checkpoint: Validation Loss {best_val_loss:.5f}, AUC {best_val_auc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping(avg_val_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Activated early stopping!\")\n",
        "        break\n",
        "\n",
        "    # Epoch time\n",
        "    epoch_time = time.time() - start_time\n",
        "    print(f\"Time for epoch: {epoch_time / 60:.2f} minutes\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SwfsjrAUTbEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Param tuning"
      ],
      "metadata": {
        "id": "XboHmj_52zsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Parameters\n",
        "param_grid = {\n",
        "    \"dropout1\": [0.2, 0.5],\n",
        "    \"dropout2\": [0.5, 0.7],\n",
        "    \"batch_norm\": [False],\n",
        "    \"layer_norm1\": [False],\n",
        "    \"layer_norm2\": [True],\n",
        "    \"backbone_dim\": [1]\n",
        "}\n",
        "\n",
        "# Create param combinations\n",
        "keys, values = zip(*param_grid.items())\n",
        "config_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]"
      ],
      "metadata": {
        "id": "tQ5ENdVp259q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path for checkpoints\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/deepfake_ds/modeep_param_search/\"\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "train_session = 21                                                             # Manually change this parameter for each training session\n",
        "\n",
        "loss_choice = \"Focal\"                                                            # Focal or BCE\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "for config in config_combinations:\n",
        "    model = MultiInputMobileNetLSTM(config).to(device)\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    if loss_choice == \"Focal\":\n",
        "        criterion = FocalLoss(alpha=0.8, gamma=2.0, reduction='mean')\n",
        "    else:\n",
        "        total_real = 826\n",
        "        total_fake = 5414\n",
        "        pos_weight = torch.tensor([total_real / total_fake], dtype=torch.float32).to(device)\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=1e-5, weight_decay=1e-2)             # Optimizer with L2 reg (weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.4, patience=1,\n",
        "    )\n",
        "\n",
        "\n",
        "    # Training configuration\n",
        "    num_epochs = 40\n",
        "    early_stopping = EarlyStopping(patience=5)                                    # Early stopping\n",
        "\n",
        "    # Metrics lists\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    val_aucs = []\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_val_auc = 0.0\n",
        "\n",
        "    print(f\"Training Session: {train_session}, Configuration:{config}\")\n",
        "    train_session += 1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "\n",
        "            # Load data\n",
        "            eyes = batch[\"eyes\"].view(-1, 3, 64, 64).to(device)\n",
        "            nose = batch[\"nose\"].view(-1, 3, 64, 64).to(device)\n",
        "            mouth = batch[\"mouth\"].view(-1, 3, 64, 64).to(device)\n",
        "            labels = batch[\"label\"].float().to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(eyes, nose, mouth)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        val_loss = 0.0\n",
        "        all_labels = []                                                             # AUC\n",
        "        all_probs = []                                                              # AUC\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for val_batch in val_loader:\n",
        "\n",
        "                # Load data\n",
        "                eyes = val_batch[\"eyes\"].view(-1, 3, 64, 64).to(device)\n",
        "                nose = val_batch[\"nose\"].view(-1, 3, 64, 64).to(device)\n",
        "                mouth = val_batch[\"mouth\"].view(-1, 3, 64, 64).to(device)\n",
        "                labels = val_batch[\"label\"].float().to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(eyes, nose, mouth)\n",
        "                val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "                # Save probabilities and labels for AUC\n",
        "                probs = torch.sigmoid(outputs)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "                # Accuracy\n",
        "                predicted = (probs > 0.5).long()\n",
        "                total_correct += (predicted == labels.long()).sum().item()\n",
        "                total_samples += labels.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = total_correct / total_samples\n",
        "        val_auc = roc_auc_score(all_labels, all_probs)                              # AUC calculation\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Validation Loss: {avg_val_loss:.4f}, \"\n",
        "              f\"Accuracy: {val_accuracy:.4f}, AUC: {val_auc:.4f}\")\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Print learning rate\n",
        "        for param_group in optimizer.param_groups:\n",
        "            print(f\"Learning Rate: {param_group['lr']}\")\n",
        "\n",
        "        # Save metrics\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        val_aucs.append(val_auc)\n",
        "\n",
        "        # Save model if it's better\n",
        "        if avg_val_loss < best_val_loss:                                            # or val_auc > best_val_auc\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_val_auc = val_auc\n",
        "            torch.save(model.state_dict(), f\"{CHECKPOINT_PATH}best_model_{train_session}.pth\")\n",
        "            print(f\"Saved checkpoint: Validation Loss {best_val_loss:.4f}, AUC {best_val_auc:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        early_stopping(avg_val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Activated early stopping!\")\n",
        "            break\n",
        "\n",
        "        # Epoch time\n",
        "        epoch_time = time.time() - start_time\n",
        "        print(f\"Time for epoch: {epoch_time / 60:.2f} minutes\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oDmLXaRq3nwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test step"
      ],
      "metadata": {
        "id": "jqnqCYkd8Nwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/deepfake_ds/modeep_best_models/\"\n",
        "train_session = 2\n",
        "\n",
        "# Load weights\n",
        "model.load_state_dict(torch.load(f\"{CHECKPOINT_PATH}best_model_{train_session}.pth\"))   # weights_only=True eventually\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for test_batch in test_loader:\n",
        "        eyes = test_batch[\"eyes\"].view(-1, 3, 64, 64).to(device)              # [batch_size, 20, 3, 64, 64] -> [batch_size*20, 3, 64, 64]\n",
        "        nose = test_batch[\"nose\"].view(-1, 3, 64, 64).to(device)\n",
        "        mouth = test_batch[\"mouth\"].view(-1, 3, 64, 64).to(device)\n",
        "        labels = test_batch[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(eyes, nose, mouth)\n",
        "        test_loss += criterion(outputs, labels).item()\n",
        "\n",
        "        predicted = (outputs > 0.5).long()\n",
        "        total_correct += (predicted == labels.long()).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "test_accuracy = total_correct / total_samples\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "AXrd5DIg8PoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stats and Graphs"
      ],
      "metadata": {
        "id": "xy7mfUpy8R8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics plot"
      ],
      "metadata": {
        "id": "eWCa6ineNNHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loss Graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label=\"Training Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Accuracy Graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# AUC Graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(val_aucs, label=\"Validation AUC\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"AUC\")\n",
        "plt.title(\"Validation AUC\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0PxfqRj2SIGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model check"
      ],
      "metadata": {
        "id": "QZgICsFVNQqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/deepfake_ds/modeep_best_models/\"\n",
        "train_session = 2\n",
        "model_path = f\"{CHECKPOINT_PATH}best_model_{train_session}.pth\"\n",
        "\n",
        "# Model load state\n",
        "model = MultiInputMobileNetLSTM()\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Conta i parametri\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Il modello ha {total_params} parametri.\")"
      ],
      "metadata": {
        "id": "xmzlJ06QNSDm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "X8zxbwS9rh85",
        "v-FFvLHX0pEc",
        "XboHmj_52zsq"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}